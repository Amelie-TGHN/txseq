"""============
Pipeline fastqc
===============

:Author: Stephen Sansom
:Release: $Id$
:Date: |today|
:Tags: Python


Overview
========

The pipeline runs the FASTQc package and post-processed the output
for downstream-visualisation.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.
CGATReport report requires a :file:`conf.py` and optionally a
file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_scrnaseq.py config


Input files
-----------

The following input tables are required.

(1) A "samples.tsv" table with the following mandatory columns:

* "sample_id": a unique identifier for the sample
* "type": either 'SE' for single end or 'PE' for paired end
* "strand": either 'none', 'forward' or 'reverse'

Sample metadata can also be stored in this table for downstream analysis for 
example with columns such as:

* "condition"
* "age"
* "sex"
* "genotype"
* "batch"

  
(2) A "libraries.tsv" table with the following mandatory columns

* "sample_id": these values must match those in the sample_id in the samples.tsv
* "lane": an integer representing the sequencing lane/unit. 
* "flow_cell": an integer representing the flow cell.
* "fastq_path": For SE libraries, the fastq file path. For PE libraries: the 
   read 1 fastq: the path for read 2 is imputed by the pipelines.

Note: When samples have been sequenced across multiple lanes, use one line
per lane. Comma-separated lane and fastq_path values are not supported. Quality
control analysis is performed at lane level; lanes will be aggregated for
quantitation.

Note: Paired-end fastq files must end with "1|2.fastq.gz" or "fastq.1|2.gz".
For paired end samples the Read 1 and Read 2 FASTQ files for the same lane 
must be located in the same folder.

The "read_1" and "read_2" columns in the sample.tsv files should contain 
the names of links present in the fastq.dir/.

If samples are sequenced over multiple lanes, simply add additional lines to the sample.tsv file: 
sequencing data will be aggregated by sample_id

The sample.tsv can contain arbitrary fields of sample metadata for use downstream.


Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements (TBC):

* FastQC

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
import re
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools


# import local pipeline utility functions
import txseq.tasks as T
import txseq.tasks.samples as samples
import txseq.tasks.readqc as readqc

# -------------------------- Pipeline Configuration -------------------------- #

# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["txseq_code_dir"] = Path(__file__).parents[1]

# ----------------------- < pipeline configuration > ------------------------ #

# if len(sys.argv) > 1:
#     if(sys.argv[1] == "config") and __name__ == "__main__":
#         sys.exit(P.main(sys.argv))


if len(sys.argv) > 1:
    if(sys.argv[1] == "make"):
        S = samples.samples(sample_tsv = PARAMS["sample_table"],
                            library_tsv = PARAMS["library_table"])

# ########################################################################### #
# ############################ Run FASTQC  ################################## #
# ########################################################################### #

def fastq_jobs():

    for seq_id in S.fastqs.keys():
    
        yield([None, os.path.join("fastqc.dir",
                                  seq_id + ".sentinel")])


@files(fastq_jobs)
def fastqc(infile, outfile):
    
    t = T.setup(infile, outfile, PARAMS)

    seq_id = os.path.basename(outfile[:-len(".sentinel")])

    out_path = os.path.join(t.outdir, seq_id)
    
    if not os.path.exists(out_path):
        os.makedirs(out_path)

    fastq_path = S.fastqs[seq_id]
    
    if PARAMS["contaminants"] != "default":
        contaminants = "-c " + PARAMS["contaminants"]
    
    if PARAMS["adaptors"] != "default":
        adaptors = "-a " + PARAMS["adaptors"]

    statement = '''fastqc 
                   -o %(out_path)s
                   --extract
                   %(contaminants)s
                   %(adaptors)s
                   %(fastq_path)s
                   > %(log_file)s
                ''' % dict(PARAMS, **t.var, **locals())
                
    P.run(statement)
    IOTools.touch_file(outfile)


@split(fastqc, ["fastqc.summary.dir/fastqc_basic_statistics.tsv.gz", 
                "fastqc.summary.dir/fastqc_*.tsv.gz"])
def summarizeFastQC(infiles, outfiles):

    t = T.setup(infiles[0], outfiles[0], PARAMS)

    all_files = []

    for infile in infiles:

        track = P.snip(infile, ".sentinel")
        all_files.extend(glob.glob(
            os.path.join(track, "*_fastqc",
                         "fastqc_data.txt")))

    dfs = readqc.read_fastqc(
        all_files)

    for key, df in dfs.items():
        fn = re.sub("basic_statistics", key, outfiles[0])
        E.info("writing to {}".format(fn))
        with IOTools.open_file(fn, "w") as outf:
            df.to_csv(outf, sep="\t", index=True)


@merge(fastqc, 
       "fastqc.summary.dir/fastqc_status_summary.tsv.gz")
def buildFastQCSummaryStatus(infiles, outfile):
    '''load FastQC status summaries into a single table.'''
    readqc.buildFastQCSummaryStatus(
        infiles,
        outfile,
        "fastqc.dir")


@jobs_limit(1) #P.get_params().get("jobs_limit_db", 1), "db")
@transform((summarizeFastQC, buildFastQCSummaryStatus),
           suffix(".tsv.gz"), ".load")
def loadFastQC(infile, outfile):
    '''load FASTQC stats into database.'''

    # a check to make sure file isnt empty
    n = 0
    with IOTools.open_file(infile) as f:
        for i, line in enumerate(f):
            n =+ i
    if n > 0:
        P.load(infile, outfile, options="--add-index=track")
    else:
        table_name = infile.replace(".tsv.gz", "")
        database_sql = P.get_params()["database"]["url"]
        database_name = os.path.basename(database_sql)
        statement = """sqlite3 %(database_name)s
                       'DROP TABLE IF EXISTS %(table_name)s;
                       CREATE TABLE %(table_name)s
                       ("track" text PRIMARY KEY, "Sequence" text,
                       "Count" integer, "Percentage" integer, "Possible Source" text);'
                       'INSERT INTO %(table_name)s VALUES ("NA", "NA", 0, 0, "NA");'"""

        P.run(statement)


# --------------------- < generic pipeline tasks > -------------------------- #


@follows(fastqc)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))