##############################################################################
#
#   Kennedy Institute of Rheumatology
#
#   $Id$
#
#   Copyright (C) 2015 Stephen Sansom
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################

"""===========================
Pipeline template
===========================

:Author: Stephen Sansom
:Release: $Id$
:Date: |today|
:Tags: Python


Overview
========

A pipeline designed to work with SMART-seq style single cell data
and bulk RNA-seq data. Single and paired end-data is supported.

This pipeline performs the follow tasks:

(1) [optional] Mapping of reads using hisat
      - paired (default) or single end fastq files are expected as the input
      - unstranded (default) or stranded fastq files are expected as the input
      - If data is already mapped position sorted, indexed BAM files can be
        provided instead.
      - See pipeline.ini and below for filename syntax guidance.

(2) Quantitation of gene expression
      - Ensembl geneset or supplied GTF (+/- spike in sequences)
      - Salmon is used to calculate TPM values
      - Cufflinks (cuffquant + cuffnorm) can be optionally run
      - featureCounts (from the subread package) is run for counting reads

(3) Calculation of post-mapping QC statistics
      - Picard CollectRnaSeqMetrics, AlignmentSummaryMetrics,
               EstimateLibraryComplexity, InsertSizeMetrics
      - Fraction spliced reads
      - Ratio spike-ins / genomic
      - Three prime bias
      - Numvers of genes detected

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.
CGATReport report requires a :file:`conf.py` and optionally a
file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_scrnaseq.py config


Input files
-----------

* Fastqs

- The pipeline expects sequence data from each cell/sample in the form
  of single or paired-end fastq files to be present in the "input_dir"
  specificed in the pipeline.ini file (default = "data.dir")

- where there are multiple Fastq file per sample, the per-sample fastqs
  should be placed in directories which end with the pattern:
  (i) ".fastq.1.gz" - for paired end data
  (ii) ".fastq.gz" - for single end data

* BAMs

- location of directory containing the BAM files is specified in the
  "input_dir" variable in pipeline.ini (default = "data.dir")

* Naming

It is recommended that files are named according
to the following convention (for plate-based single-cell data):

source_condition_replicate_plate_row_column

An arbitrary number of fields can be specified, e.g. see the
pipeline.yml default:

    name_field_titles: source,condition,plate,row,column

Name fields *must be separated by underscores* (for SQL/R compatibility).

example fastq file name:

    mTEChi_wildtype_plate1_A_1.fastq.1.gz

If the pipeline is being used to compare BAM files from different mappers,
the mapper should be supplied as an extra name field, e.g.

  name_field_titles: source,condition,plate,row,column,mapper

    mTEChi_wildtype_plate1_A_1_gsnap.bam
    mTEChi_wildtype_plate1_A_1_gsnap.bam.bai

* Spike-in information

If spike-ins are used, the location of a table containing the per-cell
spike in copy numbers should be provided in a file specified in pipeline.yml

The expected structure (tab-delimited) is:

    gene_id|copies_per_cell
    ERCC-..|20324.7225

Note that the columns headers "gene_id" and "copies_per_cell" are required.


Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements (TBC):

* Cufflinks
* Picard
* Hisat2
* subread
* R
* Salmon
* Kent tools

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
from cgatcore import database as DB

import PipelineScRnaseq as PipelineScRnaseq


# -------------------------- < parse parameters > --------------------------- #

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


# ----------------------- < pipeline configuration > ------------------------ #

if len(sys.argv) > 1:
    if(sys.argv[1] == "config") and __name__ == "__main__":
        sys.exit(P.main(sys.argv))

# Establish the location of module scripts for P.submit() functions
# set the location of the tenx code directory
if "code_dir" not in PARAMS.keys():
    code_dir = os.path.dirname(os.path.realpath(__file__))
else:
    raise ValueError("Could not set the location of the code directory")


# Set the database locations
DATABASE = PARAMS["database"]["file"]
ANN_DATABASE = PARAMS["annotations_database"]

# set the location of the scseq code directory
if "scseq_dir" not in PARAMS.keys():
    PARAMS["scseq_dir"] = Path(__file__).parents[1]
else:
    raise ValueError("Could not set the location of the scseq code directory")


# ########################################################################### #
# ############# Check sample files and prepare Sample table  ################ #
# ########################################################################### #

if PARAMS["input_type"].lower() == "fastq":
    suffix_pattern = "*.fastq.*gz"
elif PARAMS["input_type"].lower() == "bam":
    suffix_pattern = "*.bam"
else:
    raise ValueError("this pipeline only supports fastq or bam files")

SAMPLE_FILES = glob.glob(os.path.join(PARAMS["input_dir"], suffix_pattern))

# Check we have input
if len(SAMPLE_FILES) == 0:
    raise ValueError("No input files detected")

SAMPLE_IDS = set([os.path.basename(SF).split(".")[0] for SF in SAMPLE_FILES])
NAME_FIELD_TITLES = PARAMS["name_field_titles"].split(",")

# Check field names
if any([x in NAME_FIELD_TITLES for x in ("sample_id")]):
    raise ValueError('"sample_id" is reserved and cannot'
                     'be used as a name field title')

# Sanity check file names
for sample_id in SAMPLE_IDS:
    if len(sample_id.split("_")) != len(NAME_FIELD_TITLES):
        raise ValueError("%(sample_id)s does not have the expected"
                         " number of name fields (%(NAME_FIELD_TITLES)s)."
                         " Note that name fields must be separated with"
                         " underscores" % locals())

# Prepare sample table
SAMPLES = pd.DataFrame([dict(list(zip(["sample_id"] + NAME_FIELD_TITLES,
                                      [SAMPLE_ID] + SAMPLE_ID.split("_"))))
                        for SAMPLE_ID in SAMPLE_IDS])




# ########################################################################### #
# ################ Check annotation versions and genomes  ################### #
# ########################################################################### #

# Check that the genome source is set to "ucsc" or "ensembl"
GENOME_SOURCE = PARAMS["annotations_genome_source"].lower()

if not (GENOME_SOURCE == "ucsc" or GENOME_SOURCE == "ensembl"):
    raise ValueError('annotations_genome_source must be either'
                     ' "ucsc" or "ensembl" but "%(GENOME_SOURCE)s"'
                     ' was given' % locals())

# Check that given annotations and indices contain the expected
# Ensembl version number
ENSEMBL_VERSION = str(PARAMS["annotations_ensembl_version"])

if ENSEMBL_VERSION not in PARAMS["annotations_ensembl_geneset"]:
    raise ValueError("annotations geneset does not contain the"
                     " given ensembl version number")

if (PARAMS["annotations_geneset"] == "ensembl" and
        ENSEMBL_VERSION not in PARAMS["hisat_index"]):
    raise ValueError("hisat index does not contain the"
                     " given ensembl version number")

# Check that the given genome indices contain the expected
# genome name

GENOME_NAME = PARAMS["annotations_genome"]

if GENOME_NAME not in PARAMS["hisat_index"]:
    raise ValueError("hisat_index does not contain the"
                     " given genome name")

# Figure out which geneset to use for quantitation

if PARAMS["annotations_geneset"].lower() == "ensembl":
    QUANTITATION_GTF = PARAMS["annotations_ensembl_geneset"]
else:
    QUANTITATION_GTF = PARAMS["annotations_geneset"]

# Ensembl and UCSC have different contig naming conventions
# (UCSC prefixes contig names with "chr")
# Sanity check the contig patterns


@mkdir("preflight.checks.dir")
@files(os.path.join(PARAMS["annotations_genome_dir"],
                    PARAMS["annotations_genome"] + ".fasta"),
       "preflight.checks.dir/genome.contigs.txt")
def getGenomeContigs(infile, outfile):
    '''
    Extract contig names from genome FASTA file.
    '''

    statement = '''grep \\> %(infile)s
                   | sed 's/>//g'
                   | sort -u -T %(cluster_tmpdir)s
                   > %(outfile)s
                '''
    P.run(statement)


@mkdir("preflight.checks.dir")
@files(None,
       "preflight.checks.dir/hisat2.contigs.txt")
def getHisat2Contigs(infile, outfile):
    '''
    Extract contig names from HISAT2 index file.
    '''

    statement = '''hisat2-inspect -s %(hisat_index)s
                   | grep Sequence
                   | cut -f2
                   | sort -u -T %(cluster_tmpdir)s
                   > %(outfile)s
                '''

    P.run(statement)


@mkdir("preflight.checks.dir")
@files(QUANTITATION_GTF,
       "preflight.checks.dir/quantification.geneset.contigs.txt")
def getQuantitationGenesetContigs(infile, outfile):
    '''
    Extract contig names the GTF file used for quantitation.
    '''

    statement = '''zcat %(infile)s
                   | grep -v ^#
                   | cut -f1
                   | sort -u -T %(cluster_tmpdir)s
                   > %(outfile)s
                '''

    P.run(statement)


@mkdir("preflight.checks.dir")
@files(PARAMS["annotations_ensembl_geneset"],
       "preflight.checks.dir/ensembl.geneset.contigs.txt")
def getEnsemblGenesetContigs(infile, outfile):
    '''
    Extract contig names from the ENSEMBL GTF file of gene annotation.
    '''

    statement = '''zcat %(infile)s
                   | grep -v ^#
                   | cut -f1
                   | sort -u -T %(cluster_tmpdir)s
                   > %(outfile)s
                '''

    P.run(statement)


@merge([getGenomeContigs, getHisat2Contigs,
        getQuantitationGenesetContigs,
        getEnsemblGenesetContigs],
       "preflight.checks.dir/contig.report.txt")
def checkContigs(infiles, outfile):
    '''
    Check that all the annotations and indices
    share a common set of contigs.
    '''

    contig_sets = {}
    contig_ns = []

    report = open(outfile, "w")

    for infile in infiles:

        set_name = os.path.basename(infile).split(".")[0]
        contig_sets[set_name] = []

        with open(infile, "r") as contig_file:

            for line in contig_file:

                contig = line.strip()

                if contig.startswith("chr"):

                    if GENOME_SOURCE == "ucsc":
                        pass
                    else:
                        raise ValueError("ensembl contigs are not expected"
                                         " to contain chr pattern"
                                         " in %(infile)s" % locals())

                else:
                    if GENOME_SOURCE == "ensembl":
                        pass
                    else:
                        raise ValueError("ucsc chr contig pattern not found"
                                         " in %(infile)s" % locals())

                contig_sets[set_name].append(contig)

        n_contigs = len(contig_sets[set_name])
        contig_ns.append(n_contigs)

        if n_contigs == 0:
            raise ValueError("No contigs found in %(infile)s" % locals())

        report.write("#%(set_name)s: found %(n_contigs)i contigs\n" % locals())

    begin = True

    for contig_set, contigs in contig_sets.items():

        if begin:
            common = set(contigs)
            begin = False

        else:
            common = common.intersection(set(contigs))

    n_common = len(common)
    if n_common != np.max([np.min(contig_ns), 1]):

        raise ValueError("No common contigs found")

    report.write("Found %(n_common)i common contigs:\n" % locals())

    for contig in common:
        report.write(contig + "\n")

    report.close()


# ---------------------- < specific pipeline tasks > ------------------------ #

# ########################################################################### #
# #################### Generate salmon index ################################ #
# ########################################################################### #

SALMON_INDEX_NAME = ".".join(
    [os.path.basename(QUANTITATION_GTF[:-len(".gtf.gz")]),
     PARAMS["salmon_index_type"],
     str(PARAMS["salmon_index_k"])])
SALMON_INDEX = os.path.join("annotations.dir", SALMON_INDEX_NAME)


@active_if(PARAMS["input_type"] == "fastq")
@follows(mkdir("annotations.dir"), checkContigs)
@files(QUANTITATION_GTF,
       "annotations.dir/salmon_build.log")
def generateSalmonIndex(infile, outfile):
    '''Generate salmon index from annotation gtf. '''
    genome_fasta = os.path.join(PARAMS["annotations_genome_dir"],
                                PARAMS["annotations_genome_fasta"])
    index_folder = SALMON_INDEX
    job_memory = PARAMS["salmon_index_memory"]
    statement = '''fasta_out=`mktemp -p %(cluster_tmpdir)s`;
            gtf=`mktemp -p %(cluster_tmpdir)s`;
            zcat %(infile)s > $gtf ;
             gffread
            -w $fasta_out
            -g %(genome_fasta)s
            $gtf ;
            salmon index
            -t $fasta_out
            -i %(index_folder)s
            %(salmon_index_options)s
            --type %(salmon_index_type)s
            -k %(salmon_index_k)s
            &> %(outfile)s '''
    P.run(statement)



# ########################################################################### #
# ################ (2) Quantitation of gene expression #################### #
# ########################################################################### #

# ------------------------- Geneset Definition ------------------------------ #

@follows(mkdir("annotations.dir"), checkContigs)
@files(QUANTITATION_GTF,
       "annotations.dir/quantitation.geneset.gtf.gz")
def prepareQuantitationGenesetGTF(infile, outfile):
    '''
    Preparation of the GTF file used for quantitation:
    Spike-in GTF entries are optionally appended
    to the reference annotation.
    '''

    outname = outfile[:-len(".gz")]

    geneset_stat = '''zcat %(infile)s
                     > %(outname)s;
                  '''
    if SPIKES:
        spikein_geneset = PARAMS["spikein_geneset"]
        spikein_stat = '''cat %(spikein_geneset)s >> %(outname)s;
                        '''
    else:
        spikein_stat = ''

    statement = geneset_stat + spikein_stat + "gzip %(outname)s"

    P.run(statement)


@follows(mkdir("annotations.dir"))
@files(PARAMS["annotations_ensembl_geneset"],
       "annotations.dir/ensembl.geneset.flat.gz")
def prepareEnsemblGenesetFlat(infile, outfile):
    '''
    Prepare a flat version of the geneset
    for the Picard CollectRnaSeqMetrics module.
    '''

    statement = '''gtfToGenePred
                    -genePredExt
                    -geneNameAsName2
                    -ignoreGroupsWithoutExons
                    %(infile)s
                    /dev/stdout |
                    awk 'BEGIN { OFS="\\t"}
                         {print $12, $1, $2, $3, $4, $5, $6, $7, $8, $9, $10}'
                    | gzip -c
                    > %(outfile)s
                 '''

    P.run(statement)


@follows(mkdir("annotations.dir"))
@follows(prepareQuantitationGenesetGTF)
@files(PARAMS["annotations_ensembl_geneset"],
       "annotations.dir/transcript_info.txt.gz")
def tabulateTranscriptInfoFromGTF(infile, outfile):
    '''
    Tabulate selected transcript-level metadata from a GTF.
    '''
    job_memory = "10G"

    extract_fields = ",".join(['gene_id', 'transcript_id',
                               'gene_biotype', 'transcript_biotype',
                               'gene_name'])

    log_file = outfile.replace("txt.gz", "log")

    statement = '''Rscript %(scseq_dir)s/R/tabulate_transcript_information.R
                   --gtf=%(infile)s
                   --fields=%(extract_fields)s
                   --outfile=%(outfile)s
                   &> %(log_file)s
                '''
    P.run(statement)


@transform(tabulateTranscriptInfoFromGTF,
           suffix(".txt.gz"),
           ".load")
def loadEnsemblAnnotations(infile, outfile):
    '''
    Load the annotations for salmon into the project database.
    '''

    # will use ~15G RAM
    P.load(infile, outfile, options='-i "gene_id" -i "transcript_id"')


@transform(tabulateTranscriptInfoFromGTF,
           regex("(.*)/.*"),
           r"\1/tx2gene.txt")
def tx2gene(infile, outfile):
    '''
    Preparation of transcript to gene map for use by Salmon and tximport.
    '''

    tx_info = pd.read_csv(infile, sep="\t", header=0)
    tx_info = tx_info[["transcript_id", "gene_id"]]
    tx_info = tx_info[pd.notnull(tx_info["transcript_id"])]
    tx_info = tx_info.drop_duplicates()
    tx_info.to_csv(outfile, sep="\t", header=None, index=False)

    # the above is expected to be equivalent too:
    #
    # geneset_stat = '''zcat %(geneset)s
    #  | grep transcript_id
    #  | sed 's/.*gene_id "\([^"]*\)".*transcript_id "\([^"]*\)".*/\2\t\1/g'
    #  | sort -u
    #  > %(outfile)s;
    #  '''

    if SPIKES:
        spikein_tx2db = PARAMS["spikein_tx2gene"]
        spikein_stat = '''cat %(spikein_tx2gene)s
                          >> %(outfile)s;
                       '''
        statement = spikein_stat

        P.run(statement)


# Figure out which source to use for mappings between transcript_id and gene_id
if PARAMS["salmon_tx2gene"].lower() == "ensembl":
    TX2GENE = tx2gene
else:
    TX2GENE = PARAMS["salmon_tx2gene"]


@follows(mkdir("annotations.dir"), checkContigs)
@files(QUANTITATION_GTF,
       "annotations.dir/quantitation.geneset.gtf.gz")
def prepareTranscript(infile, outfile):
    '''
    Preparation of the GTF file used for quantitation:
    Spike-in GTF entries are optionally appended
    to the reference annotation.
    '''

    outname = outfile[:-len(".gz")]

    geneset_stat = '''zcat %(infile)s
                     > %(outname)s;
                  '''
    if SPIKES:
        spikein_geneset = PARAMS["spikein_geneset"]
        spikein_stat = '''cat %(spikein_geneset)s >> %(outname)s;
                        '''
    else:
        spikein_stat = ''

    statement = geneset_stat + spikein_stat + "gzip %(outname)s"

    P.run(statement)


@follows(prepareQuantitationGenesetGTF,
         prepareEnsemblGenesetFlat,
         loadEnsemblAnnotations,
         tx2gene)
def annotations():
    '''
    Annotation target.
    '''
    pass





# ---------------------- Salmon TPM calculation ----------------------------- #

@active_if(fastqMode)
@follows(mkdir("salmon.dir"), generateSalmonIndex)
@transform(glob.glob(os.path.join(PARAMS["input_dir"], fastq_pattern)),
           regex(r".*/(.*).fastq.*.gz"),
           add_inputs(TX2GENE),
           r"salmon.dir/\1.log")
def salmon(infiles, outfile):
    '''
    Per sample quantitation using salmon.
    '''

    reads_one, tx2gene = infiles
    salmon_index = SALMON_INDEX
    outname = outfile[:-len(".log")]

    if os.path.isdir(reads_one):
        reads_one = glob.glob(os.path.join(reads_one, fastq_pattern))

    else:
        reads_one = [reads_one]

    if PAIRED:
        reads_two = [x[:-len(".1.gz")] + ".2.gz" for x in reads_one]
        fastq_input = "-1 " + " ".join(reads_one) +\
                      " -2 " + " ".join(reads_two)

    else:
        fastq_input = "-r " + " ".join(reads_one)

    if not PARAMS['salmon_params'] is None:
        salmon_options = PARAMS['salmon_params']
    else:
        salmon_options = ''

    salmon_libtype = SALMON_LIBTYPE
    salmon_params = PARAMS["salmon_params"]
    job_threads = PARAMS["salmon_threads"]

    statement = '''salmon quant -i %(salmon_index)s
                                -p %(job_threads)s
                                -g %(tx2gene)s
                                %(salmon_options)s
                                -l %(salmon_libtype)s
                                %(fastq_input)s
                                -o %(outname)s
                    &> %(outfile)s;
              '''

    P.run(statement)


@active_if(fastqMode)
@merge(salmon, "salmon.dir/salmon.transcripts.load")
def loadSalmonTranscriptQuant(infiles, outfile):
    '''
    Load the salmon transcript-level results.
    '''

    tables = [x.replace(".log", "/quant.sf") for x in infiles]

    P.concatenate_and_load(tables, outfile,
                           regex_filename=".*/(.*)/quant.sf",
                           cat="sample_id",
                           options="-i Name -i sample_id",
                           job_memory=PARAMS["sql_himem"])


@active_if(fastqMode)
@merge(salmon, "salmon.dir/salmon.genes.load")
def loadSalmonGeneQuant(infiles, outfile):
    '''
    Load the salmon gene-level results.
    '''

    tables = [x.replace(".log", "/quant.genes.sf") for x in infiles]

    P.concatenate_and_load(tables, outfile,
                           regex_filename=".*/(.*)/quant.genes.sf",
                           cat="sample_id",
                           options="-i Name -i sample_id",
                           job_memory=PARAMS["sql_himem"])


@active_if(fastqMode)
@jobs_limit(1)
@transform([loadSalmonTranscriptQuant,
            loadSalmonGeneQuant],
           regex(r"(.*)/(.*).load"),
           r"\1/\2.tpms.txt")
def salmonTPMs(infile, outfile):
    '''
    Prepare a wide table of salmon TPMs (samples x transcripts|genes).
    '''

    table = P.to_table(infile)

    if "transcript" in table:
        id_name = "transcript_id"
    elif "gene" in table:
        id_name = "gene_id"
    else:
        raise ValueError("Unexpected Salmon table name")

    con = sqlite3.connect(PARAMS["database_file"])
    c = con.cursor()

    sql = '''select sample_id, Name %(id_name)s, TPM tpm
             from %(table)s
          ''' % locals()

    df = pd.read_sql(sql, con)

    df = df.pivot(id_name, "sample_id", "tpm")
    df.to_csv(outfile, sep="\t", index=True, index_label=id_name)


@active_if(fastqMode)
@transform(salmonTPMs,
           suffix(".txt"),
           ".load")
def loadSalmonTPMs(infile, outfile):
    '''
    Load a wide table of salmon TPMs in the project database.
    '''

    if "transcript" in infile:
        id_name = "transcript_id"
    elif "gene" in infile:
        id_name = "gene_id"
    else:
        raise ValueError("Unexpected Salmon table name")

    opts = "-i " + id_name

    P.load(infile, outfile, options=opts,
           job_memory=PARAMS["sql_himem"])


# For summarisation of Salmon counts, downstream use of
# tximport is recommended.
# For this the annotations.dir/tx2gene.txt file provides
# the transcript -> gene mappings.




# ---------------------- Copynumber estimation ------------------------------ #

# Copy number estimation based on spike-in sequences and Salmon TPMs.
if PARAMS["spikein_estimate_copy_numbers"] is True:
    run_copy_number_estimation = True
else:
    run_copy_number_estimation = False


@active_if(run_copy_number_estimation)
@follows(mkdir("copy.number.dir"), loadSalmonTPMs)
@files("salmon.dir/salmon.genes.tpms.txt",
       "copy.number.dir/copy_numbers.txt")
def estimateCopyNumber(infile, outfile):
    '''
    Estimate copy numbers based on standard
    curves constructed from the spike-ins.
    '''

    statement = '''Rscript %(scseq_dir)s/R/calculate_copy_number.R
                   --spikeintable=%(spikein_copy_numbers)s
                   --spikeidcolumn=gene_id
                   --spikecopynumbercolumn=copies_per_cell
                   --exprstable=%(infile)s
                   --exprsidcolumn=gene_id
                   --outfile=%(outfile)s
                '''
    P.run(statement)


@active_if(run_copy_number_estimation)
@transform(estimateCopyNumber,
           suffix(".txt"),
           ".load")
def loadCopyNumber(infile, outfile):
    '''
    Load the copy number estimations to the project database.
    '''

    P.load(infile, outfile, options='-i "gene_id"')


# ----------------------- Quantitation target ------------------------------ #

@follows(annotations, loadFeatureCounts, loadFeaturecountsTables,
         loadSalmonTPMs, loadCopyNumber)
def quantitation():
    '''
    Quantitation target.
    '''
    pass



# ------------------------- No. genes detected ------------------------------ #

@active_if(fastqMode)
@follows(mkdir("qc.dir/"), loadSalmonTPMs, loadEnsemblAnnotations)
@files("salmon.dir/salmon.genes.tpms.load",
       "qc.dir/number.genes.detected.salmon")
def numberGenesDetectedSalmon(infile, outfile):
    '''
    Count no genes detected at copynumer > 0 in each sample.
    '''

    table = P.to_table(infile)

    statement = '''select distinct s.*, i.gene_biotype
                   from %(table)s s
                   inner join transcript_info i
                   on s.gene_id=i.gene_id
                ''' % locals()

    df = DB.fetch_DataFrame(statement, DATABASE)

    melted_df = pd.melt(df, id_vars=["gene_id", "gene_biotype"])

    grouped_df = melted_df.groupby(["gene_biotype", "variable"])

    agg_df = grouped_df.agg({"value": lambda x:
                             np.sum([1 for y in x if y > 0])})
    agg_df.reset_index(inplace=True)

    count_df = pd.pivot_table(agg_df, index="variable",
                              values="value", columns="gene_biotype")
    count_df["total"] = count_df.apply(np.sum, 1)
    count_df["sample_id"] = count_df.index

    count_df.to_csv(outfile, index=False, sep="\t")


@active_if(fastqMode)
@follows(annotations)
@files(numberGenesDetectedSalmon,
       "qc.dir/qc_no_genes_salmon.load")
def loadNumberGenesDetectedSalmon(infile, outfile):
    '''
    Load the numbers of genes expressed to the project database.
    '''

    P.load(infile, outfile,
           options='-i "sample_id"')


@follows(annotations)
@files(loadFeatureCounts,
       "qc.dir/number.genes.detected.featurecounts")
def numberGenesDetectedFeatureCounts(infile, outfile):
    '''
    Count of genes detected by featureCount at counts > 0 in each sample.
    '''

    table = P.to_table(infile)

    # attach = '''attach "%(ANN_DATABASE)s" as anndb''' % globals()
    statement = '''select distinct h.*, gene_biotype
                   from %(table)s h
                   inner join transcript_info i
                   on h.gene_id=i.gene_id
               ''' % locals()

    melted_df = DB.fetch_DataFrame(statement, DATABASE)

    grouped_df = melted_df.groupby(["gene_biotype", "track"])

    agg_df = grouped_df.agg({"counts": lambda x:
                             np.sum([1 for y in x if y > 0])})
    agg_df.reset_index(inplace=True)

    count_df = pd.pivot_table(agg_df, index="track",
                              values="counts", columns="gene_biotype")
    count_df["total"] = count_df.apply(np.sum, 1)
    count_df["sample_id"] = count_df.index

    count_df.to_csv(outfile, index=False, sep="\t")


@files(numberGenesDetectedFeatureCounts,
       "qc.dir/qc_no_genes_featurecounts.load")
def loadNumberGenesDetectedFeatureCounts(infile, outfile):
    '''
    Load the numbers of genes expressed to the project database.
    '''

    P.load(infile, outfile,
           options='-i "sample_id"')







# --------------------- < generic pipeline tasks > -------------------------- #

@follows(mkdir("notebook.dir"))
@transform(glob.glob(os.path.join(os.path.dirname(__file__),
                                  "pipeline_notebooks",
                                  os.path.basename(__file__)[:-len(".py")],
                                  "*")),
           regex(r".*/(.*)"),
           r"notebook.dir/\1")
def notebooks(infile, outfile):
    '''
    Utility function to copy the notebooks from the source directory
    to the working directory.
    '''

    shutil.copy(infile, outfile)


@follows(quantitation, qc)
def full():
    pass


print(sys.argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
