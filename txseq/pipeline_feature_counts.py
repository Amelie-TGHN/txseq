"""=======================
Pipeline feature_counts
=======================

Overview
========

This pipeline counts the number of reads mapped to transcript/gene models. It uses the featureCounts algorithm from the `Subread package <https://subread.sourceforge.net>`_ .


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_feature_counts.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_feature_counts.py config


Inputs
------

1. BAM files
^^^^^^^^^^^^^

The pipeline runs against BAM files present in the "api/bam" directory.


Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

Requirements:

* Picard

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
from cgatcore import database as DB
import cgatcore.iotools as IOTools


# import local pipeline utility functions
import txseq.tasks as T
import txseq.tasks.samples as samples

# ----------------------- < pipeline configuration > ------------------------ #

# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["txseq_code_dir"] = Path(__file__).parents[1]

if len(sys.argv) > 1:
    if(sys.argv[1] == "make"):
        S = samples.samples(sample_tsv = PARAMS["sample_table"],
                            library_tsv = None)


# ---------------------- < specific pipeline tasks > ------------------------ #




# ------------------------- Geneset Definition ------------------------------ #




# ----------------------------- Read Counting ------------------------------- #

@follows(mkdir("featureCounts.dir"))
@transform(collectBAMs,
           regex(r".*/(.*).bam"),
           add_inputs(prepareQuantitationGenesetGTF),
           r"featureCounts.dir/\1.counts.gz")
def featureCounts(infiles, outfile):
    '''
    Run featureCounts.
    Note that we first need to change directory to a scratch location because
    the current dir is hard coded as the temp dir!!
    '''

    bamfile, geneset = [os.path.abspath(x) for x in infiles]
    outfile_name = os.path.abspath(outfile)

    # set featureCounts options
    featurecounts_strand = FEATURECOUNTS_STRAND

    if PAIRED:
        paired_options = "-p"
    else:
        paired_options = ""

    if PARAMS["featurecounts_options"] is None:
        featurecounts_options = ""
    else:
        featurecounts_options = PARAMS["featurecounts_options"]

    job_threads = PARAMS["featurecounts_threads"]

    statement = '''cd %(cluster_tmpdir)s;
                   gtf=`mktemp -p %(cluster_tmpdir)s`;
                   counts=`mktemp -p %(cluster_tmpdir)s`;
                   zcat %(geneset)s > $gtf;
                   featureCounts
                        -a $gtf
                        -o $counts
                        -s %(featurecounts_strand)s
                        -T %(featurecounts_threads)s
                        %(featurecounts_options)s
                        %(paired_options)s
                        %(bamfile)s;
                        cut -f1,7 $counts
                        | grep -v "#" | grep -v "Geneid"
                        | gzip -c > %(outfile_name)s;
                        rm $gtf;
                        rm $counts;
                 '''

    P.run(statement)


@merge(featureCounts,
       "featureCounts.dir/featurecounts.load")
def loadFeatureCounts(infiles, outfile):
    '''
    Combine and load count data in the project database.
    '''

    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".*/(.*).counts.gz",
                           has_titles=False,
                           cat="track",
                           header="track,gene_id,counts",
                           options='-i "gene_id"',
                           job_memory=PARAMS["sql_himem"])


@files(loadFeatureCounts,
       "featureCounts.dir/featurecounts_counts.txt")
def featurecountsGeneCounts(infile, outfile):
    '''
    Prepare a gene-by-sample table of featureCounts counts.
    '''

    table = P.to_table(infile)
    con = sqlite3.connect(PARAMS["database_file"])
    c = con.cursor()

    sql = '''select track, gene_id, counts
             from %(table)s t
          ''' % locals()

    df = pd.read_sql(sql, con)
    df = df.pivot("gene_id", "track", "counts")
    df.to_csv(outfile, sep="\t", index=True, index_label="gene_id")


@transform(featurecountsGeneCounts,
           suffix(".txt"),
           ".load")
def loadFeaturecountsTables(infile, outfile):
    '''
    Load the gene-by-sample matrix of count data in the project database.
    '''

    P.load(infile, outfile, options='-i "gene_id"')




@follows(annotations)
@files(loadFeatureCounts,
       "qc.dir/number.genes.detected.featurecounts")
def numberGenesDetectedFeatureCounts(infile, outfile):
    '''
    Count of genes detected by featureCount at counts > 0 in each sample.
    '''

    table = P.to_table(infile)

    # attach = '''attach "%(ANN_DATABASE)s" as anndb''' % globals()
    statement = '''select distinct h.*, gene_biotype
                   from %(table)s h
                   inner join transcript_info i
                   on h.gene_id=i.gene_id
               ''' % locals()

    melted_df = DB.fetch_DataFrame(statement, DATABASE)

    grouped_df = melted_df.groupby(["gene_biotype", "track"])

    agg_df = grouped_df.agg({"counts": lambda x:
                             np.sum([1 for y in x if y > 0])})
    agg_df.reset_index(inplace=True)

    count_df = pd.pivot_table(agg_df, index="track",
                              values="counts", columns="gene_biotype")
    count_df["total"] = count_df.apply(np.sum, 1)
    count_df["sample_id"] = count_df.index

    count_df.to_csv(outfile, index=False, sep="\t")


@files(numberGenesDetectedFeatureCounts,
       "qc.dir/qc_no_genes_featurecounts.load")
def loadNumberGenesDetectedFeatureCounts(infile, outfile):
    '''
    Load the numbers of genes expressed to the project database.
    '''

    P.load(infile, outfile,
           options='-i "sample_id"')


# --------------------- < generic pipeline tasks > -------------------------- #

@follows(mkdir("notebook.dir"))
@transform(glob.glob(os.path.join(os.path.dirname(__file__),
                                  "pipeline_notebooks",
                                  os.path.basename(__file__)[:-len(".py")],
                                  "*")),
           regex(r".*/(.*)"),
           r"notebook.dir/\1")
def notebooks(infile, outfile):
    '''
    Utility function to copy the notebooks from the source directory
    to the working directory.
    '''

    shutil.copy(infile, outfile)


@follows(qc)
def full():
    pass


print(sys.argv)

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
