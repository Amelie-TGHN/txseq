"""=========================
Pipeline feature_counts.py
=========================

Overview
========

This pipeline counts the number of reads mapped to transcript/gene models. It uses the featureCounts algorithm from the `Subread package <https://subread.sourceforge.net>`_ .


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_feature_counts.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_feature_counts.py config


Inputs
------

1. BAM files
^^^^^^^^^^^^^

The pipeline runs against BAM files present in the "api/bam" directory.


Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

Requirements:

* Subread

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
from cgatcore import database as DB
import cgatcore.iotools as IOTools


# import local pipeline utility functions
import txseq.tasks as T
import txseq.tasks.samples as samples

# ----------------------- < pipeline configuration > ------------------------ #

# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["txseq_code_dir"] = Path(__file__).parents[1]

if len(sys.argv) > 1:
    if(sys.argv[1] == "make"):
        S = samples.samples(sample_tsv = PARAMS["samples"],
                            library_tsv = None)
        
        # Set the database location
        DATABASE = PARAMS["sqlite"]["file"]


# ---------------------- < specific pipeline tasks > ------------------------ #

# ----------------------------- Read Counting ------------------------------- #

def count_jobs():

    for sample_id in S.samples.keys():
    
        yield([os.path.join(PARAMS["bam_path"], sample_id + ".bam"),
                os.path.join("feature.counts.dir/",
                            sample_id + ".counts.sentinel")])

@files(count_jobs)
def count(infile, sentinel):
    '''
    Run featureCounts.
    '''

    t = T.setup(infile, sentinel, PARAMS,
            cpu=PARAMS["featurecounts_threads"])

    sample_id = os.path.basename(infile)[:-len(".bam")]
    sample = S.samples[sample_id]

    # set featureCounts options
    featurecounts_strand = sample.featurecounts_strand

    if sample.paired:
        paired_options = "-p"
    else:
        paired_options = ""

    if PARAMS["featurecounts_options"] is None:
        featurecounts_options = ""
    else:
        featurecounts_options = PARAMS["featurecounts_options"]

    mktemp_template = "ctmp.featureCounts.XXXXXXXXXX"
    outfile = sentinel.replace(".sentinel", ".gz")

    geneset = os.path.join(PARAMS["txseq_annotations"],
                           "api.dir/txseq.geneset.gtf.gz")

    statement = '''counts=`mktemp -p . %(mktemp_template)s`;
                   featureCounts
                    -a %(geneset)s
                    -o $counts
                    -s %(featurecounts_strand)s
                    -T %(featurecounts_threads)s
                    %(featurecounts_options)s
                    %(paired_options)s
                    %(infile)s;
                    cut -f1,7 $counts
                    | grep -v "#" | grep -v "Geneid"
                    | gzip -c > %(outfile)s;
                    rm $counts;
                 ''' % dict(PARAMS, **t.var, **locals())

    P.run(statement)
    IOTools.touch_file(sentinel)


@merge(count,
       "feature.counts.dir/featurecounts.load")
def loadCounts(infiles, outfile):
    '''
    Combine and load count data in the project database.
    '''
    
    infiles = [x.replace(".sentinel", ".gz") for x in infiles]

    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".*/(.*).counts.gz",
                           has_titles=False,
                           cat="track",
                           header="track,gene_id,counts",
                           options='-i "gene_id"',
                           job_memory=PARAMS["sql_himem"])


@files(loadCounts,
       "feature.counts.dir/featurecounts_counts.txt")
def geneCounts(infile, outfile):
    '''
    Prepare a gene-by-sample table of featureCounts counts.
    '''

    table = P.to_table(infile)
    con = sqlite3.connect(DATABASE)
    c = con.cursor()

    sql = '''select track, gene_id, counts
             from %(table)s t
          ''' % locals()

    df = pd.read_sql(sql, con)
    df = df.pivot("gene_id", "track", "counts")
    df.to_csv(outfile, sep="\t", index=True, index_label="gene_id")


@transform(geneCounts,
           suffix(".txt"),
           ".load")
def loadGeneCounts(infile, outfile):
    '''
    Load the gene-by-sample matrix of count data in the project database.
    '''

    P.load(infile, outfile, options='-i "gene_id"')


# ----------------------- load txinfo ------------------------------ #

@files(os.path.join(PARAMS["txseq_annotations"],"api.dir/txseq.transcript.info.tsv.gz"),
       "transcript.info.load")
def loadTranscriptInfo(infile, outfile):
    '''
    Load the annotations for salmon into the project database.
    '''

    # will use ~15G RAM
    P.load(infile, outfile, options='-i "gene_id" -i "transcript_id"')


@follows(loadTranscriptInfo)
@files(loadCounts,
       "feature.counts.dir/number.genes.detected.featurecounts")
def nGenesDetected(infile, outfile):
    '''
    Count of genes detected by featureCount at counts > 0 in each sample.
    '''

    table = P.to_table(infile)

    # attach = '''attach "%(ANN_DATABASE)s" as anndb''' % globals()
    statement = '''select distinct h.*, gene_biotype
                   from %(table)s h
                   inner join transcript_info i
                   on h.gene_id=i.gene_id
               ''' % locals()

    melted_df = DB.fetch_DataFrame(statement, 
                                   DATABASE)

    grouped_df = melted_df.groupby(["gene_biotype", "track"])

    agg_df = grouped_df.agg({"counts": lambda x:
                             np.sum([1 for y in x if y > 0])})
    agg_df.reset_index(inplace=True)

    count_df = pd.pivot_table(agg_df, index="track",
                              values="counts", columns="gene_biotype")
    count_df["total"] = count_df.apply(np.sum, 1)
    count_df["sample_id"] = count_df.index

    count_df.to_csv(outfile, index=False, sep="\t")


@files(nGenesDetected,
       "feature.counts.dir/qc_no_genes_featurecounts.load")
def loadNGenesDetected(infile, outfile):
    '''
    Load the numbers of genes expressed to the project database.
    '''

    P.load(infile, outfile,
           options='-i "sample_id"')


# --------------------- < generic pipeline tasks > -------------------------- #


@follows(loadNGenesDetected)
def full():
    pass


print(sys.argv)

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
