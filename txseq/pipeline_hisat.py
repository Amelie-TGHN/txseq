"""===============
Pipeline hisat
===============

Overview
========

This pipeline quantifies gene expression from FASTQ files using `Hisat2 <http://daehwankimlab.github.io/hisat2/>`_. 


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_sample.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_hisat.py config


Inputs
------

1. Hisat2 index
^^^^^^^^^^^^^^^

 A prebuilt hisat2 index, which can be built using :doc:`pipeline_hisat_index.py </pipelines/pipeline_hisat_index>`.

2. Transcript GTF
^^^^^^^^^^^^^^^^^

Used to map transcripts to genes for gene level quantification. 

3. samples.tsv
^^^^^^^^^^^^^^
For details of this file see :doc:`pipeline_readqc.py </pipelines/pipeline_readqc>`

4. libraries.tsv
^^^^^^^^^^^^^^^^
For details of this file see :doc:`pipeline_readqc.py </pipelines/pipeline_readqc>`

The location of these files must be specified in the 'pipeline_salmon.yml' file.

Use of spike ins
----------------

TODO: add support for use of spike ins!

If spike-ins are used, the location of a table containing the per-cell
spike in copy numbers should be provided in a file specified in pipeline_salmon.yml

The expected structure (tab-delimited) is:

    gene_id|copies_per_cell
    ERCC-..|20324.7225

Note that the columns headers "gene_id" and "copies_per_cell" are required.


Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements (TBC):

* Salmon

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
from cgatcore import database as DB
import cgatcore.iotools as IOTools


# import local pipeline utility functions
import txseq.tasks as T
import txseq.tasks.samples as samples

# ----------------------- < pipeline configuration > ------------------------ #

# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["txseq_code_dir"] = Path(__file__).parents[1]

if len(sys.argv) > 1:
    if(sys.argv[1] == "make"):
        S = samples.samples(sample_tsv = PARAMS["sample_table"],
                            library_tsv = PARAMS["library_table"])


# ---------------------- < specific pipeline tasks > ------------------------ #
    
# ########################################################################### #
# ################ (1) Quantitation of gene expression #################### #
# ########################################################################### #

# if PAIRED:
#         fastq_pattern = "*.fastq.1.gz"
# else:
#         fastq_pattern = "*.fastq.gz"

# if STRAND != "none":
#     HISAT_STRAND_PARAM = "--rna-strandness %s" % HISAT_STRAND
# else:
#     HISAT_STRAND_PARAM = ""

# HISAT_THREADS = PARAMS["hisat_threads"]
# HISAT_MEMORY = str(int(PARAMS["hisat_total_mb_memory"]) //
#                    int(HISAT_THREADS)) + "M"


def hisat_first_pass_jobs():

    for sample_id in S.samples.keys():
    
        yield([None,
               os.path.join("hisat.dir", "first.pass.dir",
                            sample_id + ".sentinel"
        )])

#@transform(glob.glob(os.path.join(PARAMS["input_dir"], fastq_pattern)),
#           regex(r".*/(.*).fastq.*.gz"),
#           r"hisat.dir/first.pass.dir/\1.novel.splice.sites.txt.gz")

@files(hisat_first_pass_jobs)
def firstPass(infile, sentinel):
    '''
    Run a first hisat pass to identify novel splice sites.
    '''

    t = T.setup(infile, sentinel, PARAMS,
                memory=PARAMS["align_resources_memory"],
                cpu=PARAMS["align_resources_threads"])

    sample = S.samples[os.path.basename(sentinel)[:-len(".sentinel")]]

    if sample.paired:
        fastq_input = "-1 " + ",".join(sample.fastq["read1"]) +\
                      " -2 " + ",".join(sample.fastq["read2"])

    else:
        fastq_input = "-U " + ",".join(sample.fastq["read1"])
    
    known_ss = ""
    if PARAMS["align_splice_sites"].lower() != "false":
    
        ss_file = os.path.join(os.path.dirname(PARAMS["align_index"]),
                               PARAMS["align_splice_sites"])
    
        known_ss = "--known-splicesite-infile=" + ss_file        
    
    novel_ss_outfile = t.outname + ".novel.splice.sites.txt"

    if sample.strand != "none":
        strand_param = "--rna-strandness %s" % sample.hisat_strand
    else:
        strand_param = ""

    statement = '''hisat2
                        -x %(align_index)s
                        %(fastq_input)s
                        --threads %(align_resources_threads)s
                        %(known_ss)s
                        --novel-splicesite-outfile %(novel_ss_outfile)s
                        %(strand_param)s
                        %(align_options)s
                        -S /dev/null
                        &> %(log_file)s;
                    gzip %(novel_ss_outfile)s
                ''' % dict(PARAMS, **t.var, **locals())

    P.run(statement)
    IOTools.touch_file(sentinel) 



@follows(mkdir("annotations.dir"))
@merge(firstPass, "annotations.dir/novel.splice.sites.hisat.txt")
def novelHisatSpliceSites(infiles, outfile):
    '''
    Collect the novel splice sites into a single file.
    '''

    junction_files = " ".join(infiles)

    statement = '''zcat %(junction_files)s
                   | sort -k1,1 -T %(cluster_tmpdir)s
                   | uniq
                   > %(outfile)s
                '''

    P.run(statement)


def hisat_second_pass_jobs():

    for sample_id in S.samples.keys():
    
        yield([None,
               os.path.join("hisat.dir",
                            sample_id + ".sentinel")])

# @transform(glob.glob(os.path.join(PARAMS["input_dir"], fastq_pattern)),
#            regex(r".*/(.*).fastq.*.gz"),
#            add_inputs(novelHisatSpliceSites),
#            r"hisat.dir/\1.bam"
#            )
@follows(novelHisatSpliceSites)
@files(hisat_second_pass_jobs)
def secondPass(infiles, outfile):
    '''
    Align reads using HISAT with known and novel junctions.
    '''

    reads_one, novel_splice_sites = infiles

    if os.path.isdir(reads_one):
        reads_one = glob.glob(os.path.join(reads_one, fastq_pattern))
    else:
        reads_one = [reads_one]

    index = PARAMS["hisat_index"]
    log = outfile + ".log"


    if PAIRED:
        reads_two = [x[:-len(".1.gz")] + ".2.gz" for x in reads_one]
        fastq_input = "-1 " + ",".join(reads_one) +\
                      " -2 " + ",".join(reads_two)
    else:
        fastq_input = "-U " + ",".join(reads_one)

    hisat_strand_param = HISAT_STRAND_PARAM

    statement = '''sort_sam=`mktemp -p %(cluster_tmpdir)s`;
                   %(hisat_executable)s
                      -x %(index)s
                      %(fastq_input)s
                      --threads %(job_threads)s
                      --novel-splicesite-infile %(novel_splice_sites)s
                      %(hisat_strand_param)s
                      %(hisat_options)s
                   2> %(log)s
                   | samtools view - -bS
                   | samtools sort - -T $sort_sam -o %(outfile)s >>%(log)s;
                   samtools index %(outfile)s;
                   rm $sort_sam;
                 '''

    P.run(statement)


# --------------------- < generic pipeline tasks > -------------------------- #

@follows(secondPass)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
