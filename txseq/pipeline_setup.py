"""=================
pipeline_setup.py
=================

Overview
========

This pipeline should be run before running any of the other pipelines. It checks the input files and registers them along with genome sequence and annotation files on the API for use by downstream pipelines.


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_annotations.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_annotations.py config


Inputs
------

(1) "samples.tsv"
^^^^^^^^^^^^^^^^^

A tab-separated text file with the following mandatory columns:

* "sample_id": a unique identifier for the sample
* "type": either 'SE' for single end or 'PE' for paired end
* "strand": either 'none', 'forward' or 'reverse' (see note below).

Sample metadata can also be stored in this table for downstream analysis for 
example with columns such as:

* "condition"
* "age"
* "sex"
* "genotype"
* "batch"

.. note:: strand values of 'none', 'forward' and 'reverse' will be used to set parameter values in txseq pipelines as follows:

  * "none":  data is treated as unstranded. This is appropriate for e.g. Illumina Truseq and most single-cell protocols. :

    * hisat: default, i.e. --rna-strandedness not set
    * cufflinks: fr-secondstrand
    * HT-seq: no
    * PICARD: NONE
    * SALMON: (I)U

  * "forward": The first read (if paired) or read (if single end) corresponds to the transcript strand e.g. Directional Illumina, Standard Solid.

    * hisat: SE: F, PE: FR
    * cufflinks: fr-secondstrand
    * HT-seq: yes
    * PICARD: FIRST_READ_TRANSCRIPTION_STRAND
    * SALMON: (I)SF
    
  * "reverse": The first read (if paired) or read (if single end) corresponds to the reverse complement of the  transcript strand e.g. dUTP, NSR, NNSR

    * hisat: SE: R, PE: RF
    * cufflinks: fr-firststrand
    * HT-seq: reverse
    * PICARD: SECOND_READ_TRANSCRIPTION_STRAND
    * SALMON: (I)SR


(2) "libraries.tsv" (optional)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Required when starting from FASTQ files.

A tab-separated text file with the following mandatory columns

* "sample_id": these values must match those in the sample_id in the samples.tsv
* "lane": an integer representing the sequencing lane/unit. 
* "flow_cell": an integer representing the flow cell.
* "fastq_path": For SE libraries, the fastq file path. For PE libraries: the 
   read 1 fastq: the path for read 2 is imputed by the pipelines.

.. Note:: When samples have been sequenced across multiple lanes, use one line per lane. Comma-separated lane and fastq_path values are not supported. Quality control analysis is performed at lane level; lanes will be aggregated for quantitation.

.. Note:: Paired-end fastq files must end with "1|2.fastq.gz" or "fastq.1|2.gz". For paired end samples the Read 1 and Read 2 FASTQ files for the same lane must be located in the same folder.


(3) "bam.tsv" (optional)
^^^^^^^^^^^^^^^^^^^^^^^^

Required when starting from BAM files.

A table with the following mandatory columns

* "sample_id": these values must match those in the sample_id in the samples.tsv
* "bam_path": The BAM file path

.. Note:: Only one BAM file per sample can be provided.


(4) Annotations
^^^^^^^^^^^^^^^

The following annotation files need to be configured in the pipeline_annotations.yml file.

#. primary assembly





Requirements
------------

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

Requirements:

* Subread

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import shutil
import os
from pathlib import Path
import glob
import sqlite3

import pandas as pd
import numpy as np

from cgatcore import experiment as E
from cgatcore import pipeline as P
from cgatcore import database as DB
import cgatcore.iotools as IOTools


# import local pipeline utility functions
import txseq.tasks as T
import txseq.tasks.samples as samples

# ----------------------- < pipeline configuration > ------------------------ #

# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["txseq_code_dir"] = Path(__file__).parents[1]

if len(sys.argv) > 1:
    if(sys.argv[1] == "make"):
        S = samples.samples(sample_tsv = PARAMS["sample_table"],
                            library_tsv = None)


# ---------------------- < specific pipeline tasks > ------------------------ #

# ----------------------------- Read Counting ------------------------------- #

def count_jobs():

    for sample_id in S.samples.keys():
    
        yield([os.path.join("api", "bam", sample_id + ".bam"),
                os.path.join("feature.counts.dir/",
                            sample_id + ".counts.sentinel")])

@files(count_jobs)
def count(infile, sentinel):
    '''
    Run featureCounts.
    '''

    t = T.setup(infile, sentinel, PARAMS,
            cpu=PARAMS["featurecounts_threads"])

    sample_id = os.path.basename(infile)[:-len(".bam")]
    sample = S.samples[sample_id]

    # set featureCounts options
    featurecounts_strand = sample.featurecounts_strand

    if sample.paired:
        paired_options = "-p"
    else:
        paired_options = ""

    if PARAMS["featurecounts_options"] is None:
        featurecounts_options = ""
    else:
        featurecounts_options = PARAMS["featurecounts_options"]

    mktemp_template = "ctmp.featureCounts.XXXXXXXXXX"
    outfile = sentinel.replace(".sentinel", ".gz")

    statement = '''counts=`mktemp -p . %(mktemp_template)s`;
                   featureCounts
                    -a %(geneset)s
                    -o $counts
                    -s %(featurecounts_strand)s
                    -T %(featurecounts_threads)s
                    %(featurecounts_options)s
                    %(paired_options)s
                    %(infile)s;
                    cut -f1,7 $counts
                    | grep -v "#" | grep -v "Geneid"
                    | gzip -c > %(outfile)s;
                    rm $counts;
                 ''' % dict(PARAMS, **t.var, **locals())

    P.run(statement)
    IOTools.touch_file(sentinel)


@merge(count,
       "featureCounts.dir/featurecounts.load")
def loadCounts(infiles, outfile):
    '''
    Combine and load count data in the project database.
    '''
    
    infiles = [x.replace(".sentinel", ".gz") for x in infiles]

    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".*/(.*).counts.gz",
                           has_titles=False,
                           cat="track",
                           header="track,gene_id,counts",
                           options='-i "gene_id"',
                           job_memory=PARAMS["sql_himem"])


@files(loadCounts,
       "feature.counts.dir/featurecounts_counts.txt")
def geneCounts(infile, outfile):
    '''
    Prepare a gene-by-sample table of featureCounts counts.
    '''

    table = P.to_table(infile)
    con = sqlite3.connect(PARAMS["database_file"])
    c = con.cursor()

    sql = '''select track, gene_id, counts
             from %(table)s t
          ''' % locals()

    df = pd.read_sql(sql, con)
    df = df.pivot("gene_id", "track", "counts")
    df.to_csv(outfile, sep="\t", index=True, index_label="gene_id")


@transform(geneCounts,
           suffix(".txt"),
           ".load")
def loadGeneCounts(infile, outfile):
    '''
    Load the gene-by-sample matrix of count data in the project database.
    '''

    P.load(infile, outfile, options='-i "gene_id"')


@files(loadCounts,
       "feature.counts.dir/number.genes.detected.featurecounts")
def nGenesDetected(infile, outfile):
    '''
    Count of genes detected by featureCount at counts > 0 in each sample.
    '''

    table = P.to_table(infile)

    # attach = '''attach "%(ANN_DATABASE)s" as anndb''' % globals()
    statement = '''select distinct h.*, gene_biotype
                   from %(table)s h
                   inner join transcript_info i
                   on h.gene_id=i.gene_id
               ''' % locals()

    melted_df = DB.fetch_DataFrame(statement, 
                                   PARAMS["database_file"])

    grouped_df = melted_df.groupby(["gene_biotype", "track"])

    agg_df = grouped_df.agg({"counts": lambda x:
                             np.sum([1 for y in x if y > 0])})
    agg_df.reset_index(inplace=True)

    count_df = pd.pivot_table(agg_df, index="track",
                              values="counts", columns="gene_biotype")
    count_df["total"] = count_df.apply(np.sum, 1)
    count_df["sample_id"] = count_df.index

    count_df.to_csv(outfile, index=False, sep="\t")


@files(nGenesDetected,
       "feature.counts.dir/qc_no_genes_featurecounts.load")
def loadNGenesDetected(infile, outfile):
    '''
    Load the numbers of genes expressed to the project database.
    '''

    P.load(infile, outfile,
           options='-i "sample_id"')


# --------------------- < generic pipeline tasks > -------------------------- #


@follows(loadNGenesDetected)
def full():
    pass


print(sys.argv)

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
